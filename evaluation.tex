\section{Initial Evaluation}

% Testing a novel learning interaction, where students watch a
% CodeInk-produced trace of an algorithm, then trace the algorithm for
% themselves on new examples in the same environment
An early evaluation of our DM gesture set~\cite{Scott2014} considered the use of CodeInk by instructors and showed
%%% was there an earlier gesture set?
that they found CodeInk easier to use and more helpful than a
computer drawing application when explaining list sorting algorithms. We have also evaluated CodeInk's
viability for students, exploring two hypotheses:

\noindent \textbf{H1}: Students learn as effectively from a
CodeInk-produced explanation as they do from a standard lecture video.

\noindent \textbf{H2}: Students are able to correctly trace the learned
algorithm on a new example data structure.

\noindent \textbf{Subjects}: We recruited nine students from the
introductory programming class at our university. Before the study
began, we asked subjects to rate on a 7-point Likert scale their initial understanding of lists,
insertion sort, and merge sort, based on how
well they could explain the concept to another person. The mean age was
20 ($\sigma$=3.12), with 2 males and 7 females. The mean self-reported
understanding for lists was 2.89 ($\sigma$=2.37). 8 out of 9 students
had never seen insertion or merge sort before (mean understanding =
1.22, $\sigma$=0.67).

\noindent \textbf{Tasks and Procedures}: Each subject began the
30-minute study by watching a training video and getting familiar with
CodeInk's UI and DM gesture set. Students worked on two algorithms: insertion sort and merge sort. For each they first 
learned about the algorithm by watching a video ({\em learning phase}),
then explained its trace on a different list back to the
experimenter ({\em explanation phase}).

In the learning phase, the video was either a screencast of a
CodeInk-produced trace or an excerpt from a classroom lecture
video, where an instructor traces the sorting algorithm on a 
list on the blackboard (\fig{fig:6006-insertion}). Both 
traces covered the exact same examples. After the learning 
phase the student was asked to rate their new understanding of the
algorithm on a 7-point Likert scale. In the explanation phase, the
student used CodeInk to trace the algorithm on a new list.

% After the main task, subjects filled out an exit questionnaire, rating the
% helpfulness of three key CodeInk features: the direct manipulation language,
% visualizations of comparisons, and having the trace recorded as a list of
% Python steps.

\noindent \textbf{Results}: Our study supported \textbf{H1}. Recall that
8 out of 9 students reported they had never before seen insertion sort
or merge sort. After the learning phase, students reported that they
learned equally well from a CodeInk-produced traces (mean understanding
= 6.00) as they did from a standard lecture video (mean = 5.56). The
difference was not statistically significant (p=0.196 using a
Mann-Whitney U test). \textbf{H2} was also supported, because all
students who watched the CodeInk-produced trace could then use CodeInk
to trace both insertion sort and merge sort correctly on the new list.

%That is, both the process and final output demonstrated in their traces
%were correct.

%In other words, we found no evidence that CodeInk is any more or less effective
%than a standard online lecture video for teaching these particular algorithms.

This initial study suggests the potential for a new kind of learning
workflow, where CodeInk explanations can be embedded in online course
content. Students can learn about algorithms and practice tracing their
behavior in the same environment. Their traces could then be shared and
analyzed by teaching staff, manually or automatically, as a basis for
targeted feedback~\cite{Balzer1989} on the student's understanding.

